
In this chapter we propose a research roadmap that will allow us to fully establish the validity of our thesis.
Our experimental results have shown promising results, although this still some work to be done to make
\lang more usable in real world applications.

\section{First Major Goal: Improved Coordination Mechanisms}

As we have seen in Chapter~\ref{chapter:exp}, \lang is able to scale programs reasonably well
in multicore architectures.
The use of coordination directives also help us improve the execution time of programs, although it is not very impressive
when adding more threads to the system. Therefore, the most important goal for the rest of this thesis is to improve the
efficiency of coordination directives and associated coordination code. The most critical issue is the fact that no
coordination information is shared between threads. We need to devise a fast mechanism to share coordination directives
in order to reduce the costs associated with scheduling node execution.

Currently, we use an heap data structure to manage nodes with priorities. This data structure is not the most scalable since it is difficult to parallelize its operations.
A more scalable approach may be the skip list data structure~\cite{Sundell:2005:FLC:1073765.1073770}.

We want to implement more programs that use coordination. By implementing new programs we can
extend the set of coordination directives and find out which action or sensing facts may be
appropriate in our system. We are specially interested in designing more sensing and action
facts that place nodes in specific threads to improve data locality.

A potential algorithm that we can implement is the Alpha-Beta search algorithm. It is a good program
because we can reduce the search space by prioritizing certain tree branches. It remains to be
seen if the program can be easily implemented in \lang.
We intend to measure the execution speedup of these new coordinated programs.

\section{Second Major Goal: Improved parallelism}

The second goal is to improve the overall scalability of the system. We have tried running our programs with up to 32 threads and I've seen
some scalability problems. To accomplish this, we first need to understand how
threads communicate in the system and how that can be improved, specially in terms of node stealing and load balancing.

While the current virtual machine shows good scalability, there is still room for improvement.

Firstly, it should be possible for other threads to immediately update the set of candidate rules when a fact
is sent to a node in another thread. This will increase data locality and reduce overall execution time.

Secondly, we are currently ignoring coordination information that is sent to nodes in other threads.
This tends to reduce the effects of coordination since an increasing number of
coordination facts are being ignored when more threads are added to the system. We need to design an efficient mechanism that allows sharing of coordination
information between threads. A direct approach is to allow other threads to concurrently manage the priority queue of nodes
of other threads. This will require efficient synchronization in order to reduce the keep the costs of coordination low.

Finally, there is a plenty of small improvements that can be made in the node stealing algorithm in order to
get a slightly better scalability.

\section{Third Major Goal: Compilation and Runtime Improvements}

The third goal is to improve the absolute running time of Meld programs. While some programs fair very well against slower languages such as Python,
there's still some work that can be done to make \lang more competitive. \lang programs can take advantage of the fact that
\lang is logic-based language and thus it is possible to optimize and simplify the execution of rules
if we prove that facts follow certain properties and constraints.

Although compiled \lang programs are byte-code interpreted, which is a natural
source of overhead, we think this is still the best architecture for our system and the overhead
is on the na\"{\i}ve compilation of programs. However, 
if some rules use a lot of mathematical or list operations it may be worth trying to compile them into machine code instead of using interpreted byte-code.

To better understand the sources of overhead of our virtual machine, we propose doing a series
of experiments to understand where the most execution time is spent. We will gather the following
items:

\begin{itemize}
   \item \textbf{Database search operations}
   
   Measure the time spent fetching or searching facts from the database.
   
   \item \textbf{Database insertion/removal operations}
   
   Measure the time spent inserting and removing facts into the database.
   
   \item \textbf{Rule measurement}
   
   Measure the time spent executing each rule of the program.
   
   \item \textbf{Fact statistics}
   
   Measure the number of facts derived and consumed for each predicate.
   
   \item \textbf{Communication overhead}
   
   Measure the communication overhead between threads.

   \item \textbf{Priority information}

   Collect information about the use of coordination directives and their effect on the runtime in terms of number of facts proved and rules derived.
   
\end{itemize}

\subsection{Invariant Optimizations}

The sequential execution of LM programs can be further improved by using invariant optimizations.
By this, we mean proving certain properties about the program and its facts and then compiling the program accordingly.

\begin{itemize}
   
   \item \textbf{Property facts}
   
   In most programs there is a set of linear facts that act as a field of the node. These facts may
   be consumed and derived but they are never consumed because only their arguments change.
   For example:

\begin{Verbatim}
a(A, X), b(A, X) -o a(A, Y).
\end{Verbatim}

   In this case we consume \texttt{a(A, X)} but we derive another \texttt{a(A, Y)} with a different
   second argument. We want to prove that, for certain predicates, at any point in time there is only one
   set of facts of that predicate and although they may be consumed, only the arguments change.
   With this analysis, we can improve the code to avoid unnecessary re-derivations.
   
   \item \textbf{Triggering linear facts}
   
   Some facts never really trigger the execution of rule, because most rules need one or two special facts that
   will activate the rule. Those facts are derived in only a few rules and are intermittently consumed and
   re-derived. All the non-triggering facts never activate the execution of rules because they may be
   property facts and are constantly present in the database.
   We must prove which predicates potentially trigger the execution of rules and mark them as such.
   
   \item \textbf{Single use facts}
   
   Single use facts are a special case of triggering linear facts because once they are derived they are
   proven to be immediately consumed. The following example shows the only place in the program where
   the \texttt{a} predicate is used in the body of the rule. Note that no matter what the state of
   the database is, we can prove that \texttt{a} facts can be used immediately and will work as a function
   call in imperative languages.
   
   \begin{Verbatim}
   .... -o a(A, 2).

a(A, 1) -o ....
a(A, 2) -o ....
a(A, 3) -o ....

   ....
   \end{Verbatim}
    
\end{itemize}

Note that these are only preliminary invariant optimizations. More optimization ideas will appear
as we look further into our programs and find general patterns.

\section{Measurements}

After implementing all the needed optimizations, we want to measure the effect of each optimization
and relate each program to the optimizations used.

Finally we want to systematically compare \lang execution against execution of the identical programs
in languages such as C, Python, Haskell and Prolog. While we do not expected to run as fast as C,
\lang programs will be very competitive against Python, Haskell and Prolog. This is very important, since
if the sequential execution is competitive enough, then running the program in parallel will
easily beat such systems.

\section{Optional Major Goal: Program Correctness}

If the time allows, we also want to write correctness and termination proofs for some of the implemented \lang programs.

Some work has already been done to prove properties of linear logic programs.
Robert Simmons employed an approach called generative invariants~\cite{simmons:Thesis} to prove that an operational semantics of a programming
language follows some invariants, namely type preservation. It may be possible to extend such approach to
prove several invariants about our programs, including correctness invariants.

\section{Planned Schedule}

The proposed work is scheduled on a 18 month timeline ending May 2015.

\begin{itemize}
   \item \textbf{Spring 2014}
      
   Perform the overhead and scalability analysis for the current set of \lang programs.
   Spend some time thinking about how threads could efficiently share coordination data.

   \textbf{Checkpoint}: new benchmarks for \lang programs taking into account the new optimizations.

   \item \textbf{Summer 2014}

   Write a few more programs using coordination.   
   Meanwhile extend the set of coordination directives that will help implement the new programs.
   
   Design a set of invariant optimizations and implement them in the compiler. Meanwhile, benchmark the runtime of programs to see how they improve the execution.

   \textbf{Checkpoint}: new benchmarks for \lang programs and their comparison against other implementations.
   
   \item \textbf{Fall 2014}
   
   Continue working on invariant optimizations and scalability optimizations.
   Build website for \lang and then make \lang publicly available.
   
   \textbf{Checkpoint}: The \lang website. Improved benchmark results for \lang programs.
   
   \item \textbf{Spring 2015}
   
   Write the thesis document.
   
   \textbf{Checkpoint}: thesis defense.

\end{itemize}

