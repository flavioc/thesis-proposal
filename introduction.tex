
The last decade has seen a priority shift for processor manifactures. If clock frequency
was once the main metric for performance, today computing power is measured in number of
cores in a single chip.
For software developers and computer scientists, once focused in developing sequential programs,
newer hardware usually meant faster programs without any change to the source code. Today,
the free lunch is over. Multicore processors are now forcing the development of
new software methodologies that take advantage of increasing processing power through parallelism.
However, parallel programming is difficult, usually because programs are written
in imperative and stateful programming languages that make use of low level synchronization
primitives such as locks, mutexes and barriers. This tends to make the task of managing multithreaded
execution quite intricate and error-prone, resulting in race hazards and deadlocks.
In the future, \emph{many-core} processors will make this task look even more daunting.

Advances in network speed and bandwidth are making distributed computing
more appealing. For instance, \emph{cloud computing} is a new emerging paradigm that wants
to make every computer connected to the Internet as a client of a pool of computing power,
where data can be retrieved and computation performed. From the perspective of high performance
computing, the \emph{computer cluster} is a well established paradigm that uses fast local area
networks to improve performance and solve problems that would take a long time with a single computer.

Developments in parallel and distributed programming have given birth to several programming models.
At the end of the spectrum are lower-level programming abstractions such as
\emph{message passing} (e.g., MPI~\cite{gabriel04-open-mpi}) and \emph{shared memory}
(e.g., Pthreads~\cite{Butenhof:1997:PPT:263953} or OpenMP~\cite{Chapman-2007-UOP-1370966}).
While such abstractions are very expressive and enable the programmer to write very performant code,
they tend to be very hard to use, debug or prove correctness.
On the opposite side, we have many declarative programming models
that can be run in parallel~\cite{Blelloch:1996:PPA:227234.227246}.
While those declarative paradigms tend to make programs easier to reason about, they tend to offer
little or no control to the programmer for expressing or managing parallel execution
which may result in suboptimal performance.

In the context of the Claytronics project~\cite{goldstein-computer05}, Ashley-Rollman et
al~\cite{ashley-rollman-iclp09, ashley-rollman-derosa-iros07wksp} have created Meld, a logic programming language suited to
program massively distributed systems made of modular robots with a dynamic topology. Meld programs can derive actions that are used
to make the robots act on the outside world. The distribution of computation is done
by first partitioning the program state across the robots and then making the computation local to the node. Because Meld programs
are sets of logical clauses, they are more amenable to proof.

However, Meld and other declarative programming models give very little control to the programmer since they are heavily declarative.
This is a clear disadvantage against lower-level abstractions, since its difficult to change how programs are scheduled by
the runtime system and how the system manages parallelism.

In this proposal, we present a new programming language for parallel programming that extends the
original Meld with linear logic in order to make programs more stateful. Linear logic gives the language a structured
way to manage state, allowing the programmer to derive and delete logical facts.
While the new language retains the declarative aspects of Meld, it adds explicit programmer control
due to the introduction of linear logic and the opportunities for optimization that arise with it.

We are interested in efficiently executing graph-based algorithms on multicores. Meld is a good
starting point because it sees a distributed program as a network of processing units, therefore
there is a naturally mapping to these kinds of algorithms.
We next give an overview of the related work including declarative languages, programming models
based on graphs, linear logic, coordination and provability.

\section{Related Work}

\input{related_work}

\section{Thesis Statement}

\input{statement}

